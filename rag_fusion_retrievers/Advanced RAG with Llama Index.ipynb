{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e295474a-10bb-468a-95d2-3eeeaaf9acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d116c7b4-d15c-4ffc-ba2b-b28acbb2b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_hub.file.pymu_pdf.base import PyMuPDFReader\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a314df8e-ad76-4fb5-8652-311546aee6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72efeffe-62a3-45f1-9994-235fa05352cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afd8e3ff-bc0c-484c-88ce-b4294b380610",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1a45e8-86b4-4a29-be10-f42529909835",
   "metadata": {},
   "source": [
    "### Step-1: Query Generation / Rewriting\n",
    "\n",
    "The first step is to generate queries from the original query to better match the query intent, and increase precision/recall of the retrieved results. For instance, we might be able to rewrite the query into smaller queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5a4ff1-9c20-4b08-ba7b-c9c95e86b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2453d46b-81c2-4cc1-a63d-3680153a8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "720eec69-6c45-4f65-820c-7143c79355f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['num_queries', 'query'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='You are a helpful assistant that generates multiple search queries based on a single input query. Generate {num_queries} search queries, one on each line, related to the following input query:\\nQuery: {query}\\nQueries:\\n')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_gen_prompt_str = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "query_gen_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d15bb32-dd30-498d-8e5f-9c14dcf9f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "542162db-ad97-498d-8558-2717bad5f806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the benchmarks used to evaluate open-source chat models?',\n",
       " '2. Can you provide a comparison between the models developed in this work and existing open-source chat models?',\n",
       " '3. Are there any notable differences in performance between the models developed in this work and open-source chat models based on the benchmarks tested?']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = generate_queries(llm, query_str, num_queries=4)\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7213afd-f502-47f4-af6f-5b21caf81c47",
   "metadata": {},
   "source": [
    "## Step-2: Perform Vector Search for Each Query\n",
    "\n",
    "Now we run retrieval for each query. This means that we fetch the top-k most relevant results from each vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05234f87-f3aa-42e7-82f4-ab367d09c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bffb93d1-06b6-4243-aab4-ebd9c7e10bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get retrievers\n",
    "from llama_index.retrievers import BM25Retriever\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "891b871f-88de-4e57-be36-db76babaa1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 14.93it/s]\n"
     ]
    }
   ],
   "source": [
    "results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ced59c-ba14-46c4-bae9-d2071cfd6c4b",
   "metadata": {},
   "source": [
    "## Step-3: Perform Fusion\n",
    "\n",
    "The next step here is to perform fusion: combining the results from several retrievers into one and re-ranking.\n",
    "\n",
    "Note that a given node might be retrieved multiple times from different retrievers, so there needs to be a way to de-dup and rerank the node given the multiple retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8aabc56-44f6-43a3-a206-23e1cbea9608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_results(results_dict, similarity_top_k: int = 2):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04295064-a434-4657-a962-a2bff40fe00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = fuse_results(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97f56149-3553-4355-8b04-cbcfc643e198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 00ea8d9d-fc1f-4de8-ada6-5ab9a69fc4ee<br>**Similarity:** 0.03306010928961749<br>**Text:** Figure 1: Helpfulness human evaluation results for Llama\n",
       "2-Chat compared to other open-source and closed-source\n",
       "models. Human raters compared model generations on ~4k\n",
       "prompts consisting of both single and multi-turn prompts.\n",
       "The 95% confidence intervals for this evaluation are between\n",
       "1% and 2%. More details in Section 3.4.2. While reviewing\n",
       "these results, it is important to note that human evaluations\n",
       "can be noisy due to limitations of the prompt set, subjectivity\n",
       "of the review guidelines, s...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 2acf0a1d-6055-47c1-8a3f-a413fbd1ab04<br>**Similarity:** 0.03306010928961749<br>**Text:** Figure 12: Human evaluation results for Llama 2-Chat models compared to open- and closed-source models\n",
       "across ~4,000 helpfulness prompts with three raters per prompt.\n",
       "The largest Llama 2-Chat model is competitive with ChatGPT. Llama 2-Chat 70B model has a win rate of\n",
       "36% and a tie rate of 31.5% relative to ChatGPT. Llama 2-Chat 70B model outperforms PaLM-bison chat\n",
       "model by a large percentage on our prompt set. More results and analysis is available in Section A.3.7.\n",
       "Inter-Rater Reliability (...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.response.notebook_utils import display_source_node\n",
    "\n",
    "for n in final_results:\n",
    "    display_source_node(n, source_length=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0442a0a0-d093-4fc3-975d-e0785064ccd6",
   "metadata": {},
   "source": [
    "## Plug into RetrieverQueryEngine\n",
    "\n",
    "Now we’re ready to define this as a custom retriever, and plug it into our RetrieverQueryEngine (which does retrieval and synthesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b155117a-2a62-45b2-a0f1-0f4463a24497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import QueryBundle\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "from llama_index.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(llm, query_str, num_queries=4)\n",
    "        results = run_queries(queries, [vector_retriever, bm25_retriever])\n",
    "        final_results = fuse_results(\n",
    "            results_dict, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "786203be-669b-4629-956b-c74f6c7d921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "fusion_retriever = FusionRetriever(\n",
    "    llm, [vector_retriever, bm25_retriever], similarity_top_k=2\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(fusion_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "916d73f9-f3d1-49d7-a994-d2bb17180bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d73d3970-ff49-4e65-9620-6f8f5c84b8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The models developed in this work, specifically Llama 2-Chat models, generally perform better than existing open-source models based on the benchmarks tested. They also appear to be on par with some of the closed-source models, at least according to the human evaluations performed.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52df655-bcb1-4005-b3d8-c056d877512c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
